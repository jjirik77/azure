 <!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive ML for Computer Vision Study Tool</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #111827;
            color: #F3F4F6;
        }
        .glassmorphism {
            background: rgba(31, 41, 55, 0.5);
            backdrop-filter: blur(10px);
            border-radius: 1rem;
            border: 1px solid rgba(255, 255, 255, 0.1);
            box-shadow: 0 4px 30px rgba(0, 0, 0, 0.1);
            transition: all 0.3s ease-in-out;
        }
        .glassmorphism:hover {
            transform: translateY(-5px);
            box-shadow: 0 8px 30px rgba(0, 0, 0, 0.2);
        }
        .btn {
            padding: 0.75rem 1.5rem;
            border-radius: 0.5rem;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.2s ease;
        }
        .btn-primary {
            background-color: #3B82F6;
            color: white;
        }
        .btn-primary:hover {
            background-color: #2563EB;
        }
        .btn-secondary {
            background-color: #374151;
            color: #F3F4F6;
        }
        .btn-secondary:hover {
            background-color: #4B5563;
        }
        .quiz-option {
            border: 1px solid #4B5563;
            background-color: #1F2937;
            padding: 1rem;
            border-radius: 0.5rem;
            cursor: pointer;
            transition: all 0.2s ease;
        }
        .quiz-option:hover {
            border-color: #3B82F6;
            background-color: #374151;
        }
        .quiz-option.selected {
            border-color: #3B82F6;
            background-color: rgba(59, 130, 246, 0.3);
        }
        .quiz-option.correct {
            border-color: #10B981;
            background-color: rgba(16, 185, 129, 0.3);
        }
        .quiz-option.incorrect {
            border-color: #EF4444;
            background-color: rgba(239, 68, 68, 0.3);
        }
        #embedding-visualizer-2d {
            position: relative;
            width: 100%;
            height: 400px;
            border-radius: 0.5rem;
        }
        .embedding-point {
            position: absolute;
            width: 16px;
            height: 16px;
            border-radius: 50%;
            transition: transform 0.2s ease;
        }
        .embedding-point:hover {
            transform: scale(1.5);
        }
        .cnn-step {
            opacity: 0.4;
            transition: opacity 0.5s ease-in-out;
            border-left: 4px solid #4B5563;
            padding-left: 1rem;
        }
        .cnn-step.active {
            opacity: 1;
            border-left-color: #3B82F6;
        }
        .arrow {
            font-size: 2rem;
            color: #6B7280;
            text-align: center;
            margin: 0.5rem 0;
        }
    </style>
</head>
<body class="antialiased p-4 md:p-8">

    <header class="text-center mb-12">
        <h1 class="text-4xl md:text-6xl font-bold tracking-tight text-transparent bg-clip-text bg-gradient-to-r from-blue-400 to-purple-500">ML for Computer Vision</h1>
        <p class="mt-4 text-lg text-gray-400">An Interactive Study Tool</p>
    </header>

    <main class="space-y-12">
        <!-- Convolutional Neural Networks (CNNs) Section -->
        <section id="cnn" class="glassmorphism p-6 md:p-8">
            <h2 class="text-3xl font-bold text-white mb-4">1. Convolutional Neural Networks (CNNs)</h2>
            <p class="text-gray-300 mb-6">CNNs are the workhorses of computer vision. They use filters to find patterns in images. Let's explore how they work step-by-step.</p>
            
            <div class="grid grid-cols-1 md:grid-cols-2 gap-8 items-center">
                <!-- Interactive Diagram -->
                <div>
                    <h3 class="text-xl font-semibold mb-4 text-center">Interactive CNN Flow</h3>
                    <div id="cnn-diagram" class="space-y-2">
                        <div id="step-1" class="cnn-step">
                            <h4 class="font-bold">1. Input Image</h4>
                            <p class="text-sm text-gray-400">An image with a known label (e.g., "strawberry") is fed into the network.</p>
                        </div>
                        <div class="arrow">↓</div>
                        <div id="step-2" class="cnn-step">
                            <h4 class="font-bold">2. Convolutional Layers</h4>
                            <p class="text-sm text-gray-400">Filters slide over the image to extract features like edges, curves, and textures, creating "feature maps".</p>
                        </div>
                        <div class="arrow">↓</div>
                        <div id="step-3" class="cnn-step">
                            <h4 class="font-bold">3. Flattening</h4>
                            <p class="text-sm text-gray-400">The 2D feature maps are converted into a 1D list of numbers.</p>
                        </div>
                        <div class="arrow">↓</div>
                        <div id="step-4" class="cnn-step">
                            <h4 class="font-bold">4. Fully Connected Network</h4>
                            <p class="text-sm text-gray-400">This standard neural network analyzes the feature list to learn which features correspond to which labels.</p>
                        </div>
                        <div class="arrow">↓</div>
                        <div id="step-5" class="cnn-step">
                            <h4 class="font-bold">5. Output (Prediction)</h4>
                            <p class="text-sm text-gray-400">The network outputs probabilities for each possible class (e.g., 95% strawberry, 4% raspberry, 1% cherry).</p>
                        </div>
                    </div>
                    <div class="mt-6 text-center">
                        <button id="animate-cnn-btn" class="btn btn-primary">Animate Flow</button>
                    </div>
                </div>

                <!-- Filter Playground -->
                <div>
                    <h3 class="text-xl font-semibold mb-4 text-center">Filter Playground</h3>
                    <div class="flex justify-center mb-4">
                        <img id="source-image" src="https://picsum.photos/id/1080/256/256" alt="A strawberry next to a cup of coffee" class="rounded-lg shadow-md" crossOrigin="anonymous">
                    </div>
                    <div class="flex justify-center mb-4">
                        <canvas id="feature-map-canvas" width="256" height="256" class="rounded-lg shadow-inner bg-gray-800"></canvas>
                    </div>
                    <div class="flex flex-wrap justify-center gap-2">
                        <button class="btn btn-secondary filter-btn" data-filter="identity">Identity</button>
                        <button class="btn btn-secondary filter-btn" data-filter="edge">Edge Detect</button>
                        <button class="btn btn-secondary filter-btn" data-filter="sharpen">Sharpen</button>
                        <button class="btn btn-secondary filter-btn" data-filter="blur">Blur</button>
                    </div>
                </div>
            </div>
        </section>

        <!-- Transformers Section -->
        <section id="transformers" class="glassmorphism p-6 md:p-8">
            <h2 class="text-3xl font-bold text-white mb-4">2. Transformers & Multi-modal Models</h2>
            <p class="text-gray-300 mb-6">Transformers, originally from language processing, create "embeddings" - vector representations of concepts. This allows multi-modal models to understand relationships between text and images.</p>
            
            <div class="grid grid-cols-1 md:grid-cols-2 gap-8 items-center">
                 <div>
                    <h3 class="text-xl font-semibold mb-4 text-center">2D Embedding Visualizer</h3>
                    <p class="text-gray-400 text-sm text-center mb-2">Hover over points to see their labels. Notice how similar items cluster together.</p>
                    <div id="embedding-visualizer-2d" class="bg-gray-900 border border-gray-700">
                        <!-- 2D points will be injected here by JS -->
                    </div>
                </div>
                <div>
                    <h3 class="text-xl font-semibold mb-4 text-center">Multi-modal Task: Captioning</h3>
                    <p class="text-gray-300 mb-4">A multi-modal model can generate captions. Which caption best describes the image?</p>
                    <img src="https://picsum.photos/id/237/400/250" alt="A black puppy" class="rounded-lg shadow-md mx-auto mb-4">
                    <div class="space-y-3">
                        <div class="caption-option p-3 rounded-lg border-2 border-gray-600 cursor-pointer hover:bg-gray-700">A cat sleeping on a couch.</div>
                        <div class="caption-option p-3 rounded-lg border-2 border-gray-600 cursor-pointer hover:bg-gray-700" data-correct="true">A dog sitting on a wooden surface.</div>
                        <div class="caption-option p-3 rounded-lg border-2 border-gray-600 cursor-pointer hover:bg-gray-700">A city skyline at night.</div>
                    </div>
                    <div id="caption-feedback" class="mt-3 text-center font-semibold"></div>
                </div>
            </div>
        </section>

        <!-- Quiz Section -->
        <section id="quiz" class="glassmorphism p-6 md:p-8">
            <h2 class="text-3xl font-bold text-white mb-6">3. Knowledge Check</h2>
            <div id="quiz-container">
                <!-- Quiz questions will be injected here by JS -->
            </div>
            <div class="mt-6 text-center">
                <button id="submit-quiz-btn" class="btn btn-primary">Check Answers</button>
                <p id="quiz-result" class="mt-4 font-bold"></p>
            </div>
        </section>

    </main>

    <footer class="text-center mt-12 text-gray-500">
        <p>An interactive study tool by Gemini.</p>
    </footer>

    <!-- Tooltip moved to the end of the body for robust positioning -->
    <div id="tooltip" class="fixed bg-white text-black p-2 rounded shadow-lg pointer-events-none" style="display: none; z-index: 9999;"></div>

    <script>
    document.addEventListener('DOMContentLoaded', () => {
        // --- CNN Animation ---
        const animateCnnBtn = document.getElementById('animate-cnn-btn');
        const cnnSteps = document.querySelectorAll('.cnn-step');
        let currentStep = 0;
        let animationInterval;

        function runAnimation() {
            cnnSteps.forEach(step => step.classList.remove('active'));
            currentStep = 0;
            
            animationInterval = setInterval(() => {
                if (currentStep < cnnSteps.length) {
                    cnnSteps[currentStep].classList.add('active');
                    currentStep++;
                } else {
                    clearInterval(animationInterval);
                }
            }, 1000);
        }
        animateCnnBtn.addEventListener('click', () => {
             clearInterval(animationInterval);
             runAnimation();
        });
        // Start animation on load
        runAnimation();


        // --- Filter Playground ---
        const sourceImage = document.getElementById('source-image');
        const canvas = document.getElementById('feature-map-canvas');
        const ctx = canvas.getContext('2d', { willReadFrequently: true });

        const kernels = {
            identity: [
                [0, 0, 0],
                [0, 1, 0],
                [0, 0, 0]
            ],
            edge: [
                [0, 1, 0],
                [1, -4, 1],
                [0, 1, 0]
            ],
            sharpen: [
                [0, -1, 0],
                [-1, 5, -1],
                [0, -1, 0]
            ],
            blur: [
                [1/9, 1/9, 1/9],
                [1/9, 1/9, 1/9],
                [1/9, 1/9, 1/9]
            ]
        };

        function applyFilter(kernel) {
            const width = sourceImage.width;
            const height = sourceImage.height;
            canvas.width = width;
            canvas.height = height;

            // Draw image to a temporary canvas to get pixel data
            const tempCanvas = document.createElement('canvas');
            const tempCtx = tempCanvas.getContext('2d');
            tempCanvas.width = width;
            tempCanvas.height = height;
            tempCtx.drawImage(sourceImage, 0, 0, width, height);
            const imageData = tempCtx.getImageData(0, 0, width, height);
            const srcPixels = imageData.data;
            const dstImageData = ctx.createImageData(width, height);
            const dstPixels = dstImageData.data;

            // Apply convolution
            for (let y = 1; y < height - 1; y++) {
                for (let x = 1; x < width - 1; x++) {
                    let r = 0, g = 0, b = 0;
                    for (let ky = -1; ky <= 1; ky++) {
                        for (let kx = -1; kx <= 1; kx++) {
                            const i = ((y + ky) * width + (x + kx)) * 4;
                            const weight = kernel[ky + 1][kx + 1];
                            r += srcPixels[i] * weight;
                            g += srcPixels[i + 1] * weight;
                            b += srcPixels[i + 2] * weight;
                        }
                    }
                    const dstIndex = (y * width + x) * 4;
                    dstPixels[dstIndex] = r;
                    dstPixels[dstIndex + 1] = g;
                    dstPixels[dstIndex + 2] = b;
                    dstPixels[dstIndex + 3] = 255; // Alpha
                }
            }
            ctx.putImageData(dstImageData, 0, 0);
        }

        sourceImage.onload = () => {
            applyFilter(kernels.identity);
        };
        // Handle potential cross-origin issues if image is not fully loaded
        if (sourceImage.complete && sourceImage.naturalHeight !== 0) {
            sourceImage.onload();
        }


        document.querySelectorAll('.filter-btn').forEach(btn => {
            btn.addEventListener('click', (e) => {
                const filterName = e.target.dataset.filter;
                applyFilter(kernels[filterName]);
            });
        });

        // --- 2D Embedding Visualizer ---
        const container2D = document.getElementById('embedding-visualizer-2d');
        const tooltip = document.getElementById('tooltip');

        // Data points: [x%, y%, label, color]
        const pointsData2D = [
            // Fruits
            { x: 20, y: 25, label: 'Apple', color: '#EF4444' },
            { x: 15, y: 30, label: 'Orange', color: '#F97316' },
            { x: 25, y: 20, label: 'Banana', color: '#FDE047' },
            // Vehicles
            { x: 80, y: 75, label: 'Car', color: '#3B82F6' },
            { x: 85, y: 80, label: 'Truck', color: '#60A5FA' },
            { x: 75, y: 70, label: 'Bus', color: '#2563EB' },
            // Animals
            { x: 25, y: 80, label: 'Dog', color: '#10B981' },
            { x: 30, y: 85, label: 'Cat', color: '#34D399' },
            { x: 20, y: 75, label: 'Lion', color: '#059669' },
        ];

        pointsData2D.forEach(p => {
            const pointEl = document.createElement('div');
            pointEl.className = 'embedding-point';
            pointEl.style.left = `${p.x}%`;
            pointEl.style.top = `${p.y}%`;
            pointEl.style.backgroundColor = p.color;
            
            pointEl.addEventListener('mouseover', (e) => {
                tooltip.style.display = 'block';
                tooltip.textContent = p.label;
                // Use clientX/clientY for fixed positioning
                tooltip.style.left = `${e.clientX + 15}px`;
                tooltip.style.top = `${e.clientY + 15}px`;
            });
            
            pointEl.addEventListener('mousemove', (e) => {
                // Use clientX/clientY for fixed positioning
                tooltip.style.left = `${e.clientX + 15}px`;
                tooltip.style.top = `${e.clientY + 15}px`;
            });

            pointEl.addEventListener('mouseout', () => {
                tooltip.style.display = 'none';
            });

            container2D.appendChild(pointEl);
        });

        // --- Multi-modal Captioning ---
        document.querySelectorAll('.caption-option').forEach(option => {
            option.addEventListener('click', (e) => {
                const feedbackEl = document.getElementById('caption-feedback');
                if (e.target.dataset.correct) {
                    feedbackEl.textContent = 'Correct! This model correctly linked the image features to the text.';
                    feedbackEl.className = 'mt-3 text-center font-semibold text-green-400';
                } else {
                    feedbackEl.textContent = 'Not quite. Try another option.';
                    feedbackEl.className = 'mt-3 text-center font-semibold text-red-400';
                }
            });
        });

        // --- Quiz ---
        const quizData = [
            {
                question: "What is the primary purpose of filters (kernels) in a CNN?",
                options: [
                    "To resize the image",
                    "To extract features like edges and textures",
                    "To change the image colors",
                    "To add a caption to the image"
                ],
                answer: 1
            },
            {
                question: "What type of neural network architecture, originally used for NLP, has been successfully applied to computer vision to create multi-modal models?",
                options: [
                    "Recurrent Neural Network (RNN)",
                    "Generative Adversarial Network (GAN)",
                    "Transformer",
                    "Autoencoder"
                ],
                answer: 2
            },
            {
                question: "In the context of transformers, what is an 'embedding'?",
                options: [
                    "A type of image filter",
                    "The final prediction of the model",
                    "A vector of numbers representing a word or concept",
                    "A layer that flattens data"
                ],
                answer: 2
            }
        ];

        const quizContainer = document.getElementById('quiz-container');
        quizData.forEach((q, index) => {
            const questionEl = document.createElement('div');
            questionEl.className = 'mb-6';
            questionEl.innerHTML = `<h4 class="font-semibold mb-3">${index + 1}. ${q.question}</h4>`;
            
            const optionsContainer = document.createElement('div');
            optionsContainer.className = 'space-y-2';
            optionsContainer.dataset.questionIndex = index;

            q.options.forEach((option, optionIndex) => {
                const optionEl = document.createElement('div');
                optionEl.className = 'quiz-option';
                optionEl.textContent = option;
                optionEl.dataset.optionIndex = optionIndex;
                optionEl.addEventListener('click', () => {
                    // Deselect siblings
                    optionsContainer.querySelectorAll('.quiz-option').forEach(o => o.classList.remove('selected'));
                    // Select clicked
                    optionEl.classList.add('selected');
                });
                optionsContainer.appendChild(optionEl);
            });
            questionEl.appendChild(optionsContainer);
            quizContainer.appendChild(questionEl);
        });

        document.getElementById('submit-quiz-btn').addEventListener('click', () => {
            let score = 0;
            quizData.forEach((q, index) => {
                const container = quizContainer.querySelector(`[data-question-index="${index}"]`);
                const selectedOption = container.querySelector('.quiz-option.selected');
                if (selectedOption) {
                    const selectedIndex = parseInt(selectedOption.dataset.optionIndex);
                    if (selectedIndex === q.answer) {
                        score++;
                        selectedOption.classList.add('correct');
                        selectedOption.classList.remove('selected');
                    } else {
                        selectedOption.classList.add('incorrect');
                        // Also show the correct one
                        container.querySelector(`[data-option-index="${q.answer}"]`).classList.add('correct');
                    }
                }
            });
            const resultEl = document.getElementById('quiz-result');
            resultEl.textContent = `You scored ${score} out of ${quizData.length}!`;
            if(score === quizData.length) {
                resultEl.className = 'mt-4 font-bold text-green-400';
            } else {
                resultEl.className = 'mt-4 font-bold text-blue-400';
            }
        });
    });
    </script>
</body>
</html>
